{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju4Th0-2PrVG"
      },
      "source": [
        "# Running our Object Detection Model\n",
        "Srihari Krishnaswamy, 5-24-2023\n",
        "\n",
        "This colab notebook runs our Yolov5 object detection model on the videos that we input. We can choose what videos are processed and what weights are used for detection by updating variables set in the notebook.\n",
        "\n",
        "Click [here](https://colab.research.google.com/drive/1tA7fIs-hGEBu-z6XTyebifMSZLuCg3mB?usp=sharing) to open this notebook in google colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlgv4AMbQ4Mw"
      },
      "source": [
        "### Setup\n",
        "First, we clone our project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0eu6Y9d_GrW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/srihariKrishnaswamy/ML-Challenge.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVwtiAP1RAQ3"
      },
      "source": [
        "### Mounting Google Drive: \n",
        "This will come in handy when downloading processed videos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJCMFFbUkJlv",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl0RqdvORf27"
      },
      "source": [
        "### Installing LFS\n",
        "The models and videos in our project are too big to be stored in git regularly, so we need LFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shILfHvi_dZN",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%cd ML-Challenge\n",
        "!git lfs install; git lfs fetch; git lfs pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgvn_xIRRt4M"
      },
      "source": [
        "### Installing Project Dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghZWqHyMARaE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip3 install -qr requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IxCHvKkSGPl"
      },
      "source": [
        "### Choosing our video and the model we want to use for detections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAlYd213A1yR",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "video1 = \"descent.mp4\" \n",
        "video2 = \"seafloor.mp4\"\n",
        "video3 = \"exp1.mp4\"\n",
        "video4 = \"exp2.mp4\"\n",
        "model = \"SeaScout.pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC2kWMcEWFZC"
      },
      "source": [
        "### Running our inference!\n",
        "This statement takes in the videos to process and the model to process them, outputting processed videos inside the latest folder inside the output folder. This file is the subprocess that gets run inside the UI to a) track organisims using the yolov5 model and b) write the detections to a spreadsheet, so this is a direct and intuitive look into how our project works!\n",
        "\n",
        "Usage: create variables for however many videos you'd like to process by setting the variable to the name of the video. The videos have to be inside the videos folder inside the project folder. This is also the case with the model. To apply detection onto the videos, enter them in the same format as below into the statement invoking the master_detect_data.py file. Please make sure that file names are valid!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNlXHykFXaIk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!python master_detect_data.py --videos {video1} {video2} {video3} {video4} --model {model}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldlqYTXJUBGK"
      },
      "source": [
        "### Now download the videos and spreadsheet from the latest output folder!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
